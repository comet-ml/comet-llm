{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Opik's Hallucination Metric\n",
    "\n",
    "For this guide we will be evaluating the Hallucination metric included in the LLM Evaluation SDK which will showcase both how to use the `evaluation` functionality in the platform as well as the quality of the Hallucination metric included in the SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an account on Comet.com\n",
    "\n",
    "[Comet](https://www.comet.com/site) provides a hosted version of the Opik platform, [simply create an account](https://www.comet.com/signup?from=llm) and grab you API Key.\n",
    "\n",
    "> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/self_hosting_opik/) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"OPIK_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPIK_API_KEY\"] = getpass.getpass(\"Opik API Key: \")\n",
    "if \"OPIK_WORKSPACE\" not in os.environ:\n",
    "    os.environ[\"OPIK_WORKSPACE\"] = input(\"Comet workspace (often the same as your username): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running the Opik platform locally, simply set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"OPIK_URL_OVERRIDE\"] = \"http://localhost:5173/api\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our environment\n",
    "\n",
    "First, we will install the necessary libraries, configure the OpenAI API key and create a new Opik dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opik pyarrow fsspec huggingface_hub --upgrade --quiet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the [HaluBench dataset](https://huggingface.co/datasets/PatronusAI/HaluBench?library=pandas) which according to this [paper](https://arxiv.org/pdf/2407.08488) GPT-4o detects 87.9% of hallucinations. The first step will be to create a dataset in the platform so we can keep track of the results of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "from opik import Opik, DatasetItem\n",
    "import pandas as pd\n",
    "\n",
    "client = Opik()\n",
    "\n",
    "try:\n",
    "    # Create dataset\n",
    "    dataset = client.create_dataset(name=\"HaluBench\", description=\"HaluBench dataset\")\n",
    "\n",
    "    # Insert items into dataset\n",
    "    df = pd.read_parquet(\"hf://datasets/PatronusAI/HaluBench/data/test-00000-of-00001.parquet\")\n",
    "    df = df.sample(n=500, random_state=42)\n",
    "\n",
    "    dataset_records = [\n",
    "        DatasetItem(\n",
    "            input = {\n",
    "                \"input\": x[\"question\"],\n",
    "                \"context\": [x[\"passage\"]],\n",
    "                \"output\": x[\"answer\"]\n",
    "            },\n",
    "            expected_output = {\"expected_output\": x[\"label\"]}\n",
    "        )\n",
    "        for x in df.to_dict(orient=\"records\")\n",
    "    ]\n",
    "    \n",
    "    dataset.insert(dataset_records)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the hallucination metric\n",
    "\n",
    "We can use the Opik SDK to compute a hallucination score for each item in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.evaluation.metrics import Hallucination\n",
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.metrics import base_metric, score_result\n",
    "from opik import Opik, DatasetItem\n",
    "import pandas as pd\n",
    "\n",
    "client = Opik()\n",
    "\n",
    "class CheckHallucinated(base_metric.BaseMetric):\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "\n",
    "    def score(self, hallucination_score, expected_hallucination_score, **kwargs):\n",
    "        return score_result.ScoreResult(\n",
    "            value= None if hallucination_score is None else hallucination_score == expected_hallucination_score,\n",
    "            name=self.name,\n",
    "            reason=f\"Got the hallucination score of {hallucination_score} and expected {expected_hallucination_score}\",\n",
    "            scoring_failed=hallucination_score is None\n",
    "        )\n",
    "\n",
    "def evaluation_task(x: DatasetItem):\n",
    "    metric = Hallucination()\n",
    "    try:\n",
    "        metric_score = metric.score(\n",
    "            input= x.input[\"input\"],\n",
    "            context= x.input[\"context\"],\n",
    "            output= x.input[\"output\"]\n",
    "        )\n",
    "        hallucination_score = metric_score.value\n",
    "        hallucination_reason = metric_score.reason\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        hallucination_score = None\n",
    "        hallucination_reason = str(e)\n",
    "    \n",
    "    return {\n",
    "        \"hallucination_score\": \"FAIL\" if hallucination_score == 1 else \"PASS\",\n",
    "        \"hallucination_reason\": hallucination_reason,\n",
    "        \"expected_hallucination_score\": x.expected_output[\"expected_output\"]\n",
    "    }\n",
    "\n",
    "dataset = client.get_dataset(name=\"HaluBench\")\n",
    "\n",
    "res = evaluate(\n",
    "    experiment_name=\"Check Comet Metric\",\n",
    "    dataset=dataset,\n",
    "    task=evaluation_task,\n",
    "    scoring_metrics=[CheckHallucinated(name=\"Detected hallucination\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the hallucination metric is able to detect ~80% of the hallucinations contained in the dataset and we can see the specific items where hallucinations were not detected.\n",
    "\n",
    "![Hallucination Evaluation](https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/static/img/cookbook/hallucination_metric_cookbook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
