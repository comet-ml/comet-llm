{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Opik's Moderation Metric\n",
    "\n",
    "*This cookbook was created from a Jypyter notebook which can be found [here](TBD).*\n",
    "\n",
    "For this guide we will be evaluating the Moderation metric included in the LLM Evaluation SDK which will showcase both how to use the `evaluation` functionality in the platform as well as the quality of the Moderation metric included in the SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an account on Comet.com\n",
    "\n",
    "[Comet](https://www.comet.com/site) provides a hosted version of the Opik platform, [simply create an account](https://www.comet.com/signup?from=llm) and grab you API Key.\n",
    "\n",
    "> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/self_hosting_opik/) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPIK_API_KEY\"] = getpass.getpass(\"Opik API Key: \")\n",
    "os.environ[\"OPIK_WORKSPACE\"] = input(\"Comet workspace (often the same as your username): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running the Opik platform locally, simply set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "# os.environ[\"OPIK_URL_OVERRIDE\"] = \"http://localhost:5173/api\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our environment\n",
    "\n",
    "First, we will install the necessary libraries and configure the OpenAI API key and download a reference moderation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the [OpenAI Moderation API Release dataset](https://github.com/openai/moderation-api-release/tree/main/data) which according to this [blog post](https://openai.com/index/using-gpt-4-for-content-moderation/) GPT-4o detects ~60~% of hallucinations. The first step will be to create a dataset in the platform so we can keep track of the results of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "from opik import Opik, DatasetItem\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "client = Opik()\n",
    "try:\n",
    "    # Create dataset\n",
    "    dataset = client.create_dataset(name=\"OpenAIModerationDataset\", description=\"OpenAI Moderation Dataset\")\n",
    "\n",
    "    # Insert items into dataset\n",
    "    url = \"https://github.com/openai/moderation-api-release/raw/main/data/samples-1680.jsonl.gz\"\n",
    "    response = requests.get(url)\n",
    "    df = pd.read_json(BytesIO(response.content), lines=True, compression='gzip')\n",
    "\n",
    "    df = df.sample(n=500, random_state=42)\n",
    "    \n",
    "    dataset_records = []\n",
    "    for x in df.to_dict(orient=\"records\"):\n",
    "        moderation_fields = [\"S\", \"H\", \"V\", \"HR\", \"SH\", \"S3\", \"H2\", \"V2\"]\n",
    "        moderated_fields = [field for field in moderation_fields if x[field] == 1.0]\n",
    "        expected_output = \"moderated\" if moderated_fields else \"not_moderated\"\n",
    "\n",
    "        dataset_records.append(\n",
    "            DatasetItem(\n",
    "                input = {\n",
    "                    \"input\": x[\"prompt\"]\n",
    "                },\n",
    "                expected_output = {\n",
    "                    \"expected_output\": expected_output,\n",
    "                    \"moderated_fields\": moderated_fields\n",
    "                }\n",
    "            ))\n",
    "    \n",
    "    dataset.insert(dataset_records)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the moderation metric\n",
    "\n",
    "We can use the Opik SDK to compute a moderation score for each item in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.evaluation.metrics import Moderation\n",
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.metrics import base_metric, score_result\n",
    "from opik import Opik, DatasetItem\n",
    "\n",
    "client = Opik()\n",
    "\n",
    "class CheckModerated(base_metric.BaseMetric):\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "\n",
    "    def score(self, moderation_score, moderation_reason, expected_moderation_score, **kwargs):\n",
    "        moderation_score = \"moderated\" if moderation_score > 0.5 else \"not_moderated\"\n",
    "\n",
    "        return score_result.ScoreResult(\n",
    "            value= None if moderation_score is None else moderation_score == expected_moderation_score,\n",
    "            name=self.name,\n",
    "            reason=f\"Got the moderation score of {moderation_score} and expected {expected_moderation_score}\",\n",
    "            scoring_failed=moderation_score is None\n",
    "        )\n",
    "\n",
    "def evaluation_task(x: DatasetItem):\n",
    "    metric = Moderation()\n",
    "    try:\n",
    "        metric_score = metric.score(\n",
    "            input= x.input[\"input\"]\n",
    "        )\n",
    "        moderation_score = metric_score.value\n",
    "        moderation_reason = metric_score.reason\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        moderation_score = None\n",
    "        moderation_reason = str(e)\n",
    "    \n",
    "    return {\n",
    "        \"moderation_score\": moderation_score,\n",
    "        \"moderation_reason\": moderation_reason,\n",
    "        \"expected_moderation_score\": x.expected_output[\"expected_output\"]\n",
    "    }\n",
    "\n",
    "dataset = client.get_dataset(name=\"OpenAIModerationDataset\")\n",
    "\n",
    "res = evaluate(\n",
    "    experiment_name=\"Check Comet Metric\",\n",
    "    dataset=dataset,\n",
    "    task=evaluation_task,\n",
    "    scoring_metrics=[CheckModerated(name=\"Detected Moderation\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to detect ~85% of moderation violations, this can be improved further by providing some additional examples to the model. We can view a breakdown of the results in the Opik UI:\n",
    "\n",
    "![Moderation Evaluation](/img/cookbook/moderation_metric_cookbook.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
