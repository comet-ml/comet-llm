{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Opik's Moderation Metric\n",
    "\n",
    "*This cookbook was created from a Jypyter notebook which can be found [here](TBD).*\n",
    "\n",
    "For this guide we will be evaluating the Moderation metric included in the LLM Evaluation SDK which will showcase both how to use the `evaluation` functionality in the platform as well as the quality of the Moderation metric included in the SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an account on Comet.com\n",
    "\n",
    "[Comet](https://www.comet.com/site) provides a hosted version of the Opik platform, [simply create an account](https://www.comet.com/signup?from=llm) and grab you API Key.\n",
    "\n",
    "> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/self_hosting_opik/) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"OPIK_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPIK_API_KEY\"] = getpass.getpass(\"Opik API Key: \")\n",
    "if \"OPIK_WORKSPACE\" not in os.environ:\n",
    "    os.environ[\"OPIK_WORKSPACE\"] = input(\"Comet workspace (often the same as your username): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running the Opik platform locally, simply set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "# os.environ[\"OPIK_URL_OVERRIDE\"] = \"http://localhost:5173/api\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our environment\n",
    "\n",
    "First, we will install the necessary libraries and configure the OpenAI API key and download a reference moderation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opik --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the [OpenAI Moderation API Release dataset](https://github.com/openai/moderation-api-release/tree/main/data) which according to this [blog post](https://openai.com/index/using-gpt-4-for-content-moderation/) GPT-4o detects ~60~% of hallucinations. The first step will be to create a dataset in the platform so we can keep track of the results of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status_code: 409, body: {'errors': ['Dataset already exists']}\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "from opik import Opik, DatasetItem\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "client = Opik()\n",
    "try:\n",
    "    # Create dataset\n",
    "    dataset = client.create_dataset(name=\"OpenAIModerationDataset\", description=\"OpenAI Moderation Dataset\")\n",
    "\n",
    "    # Insert items into dataset\n",
    "    url = \"https://github.com/openai/moderation-api-release/raw/main/data/samples-1680.jsonl.gz\"\n",
    "    response = requests.get(url)\n",
    "    df = pd.read_json(BytesIO(response.content), lines=True, compression='gzip')\n",
    "\n",
    "    df = df.sample(n=50, random_state=42)\n",
    "    \n",
    "    dataset_records = []\n",
    "    for x in df.to_dict(orient=\"records\"):\n",
    "        moderation_fields = [\"S\", \"H\", \"V\", \"HR\", \"SH\", \"S3\", \"H2\", \"V2\"]\n",
    "        moderated_fields = [field for field in moderation_fields if x[field] == 1.0]\n",
    "        expected_output = \"moderated\" if moderated_fields else \"not_moderated\"\n",
    "\n",
    "        dataset_records.append(\n",
    "            DatasetItem(\n",
    "                input = {\n",
    "                    \"input\": x[\"prompt\"]\n",
    "                },\n",
    "                expected_output = {\n",
    "                    \"expected_output\": expected_output,\n",
    "                    \"moderated_fields\": moderated_fields\n",
    "                }\n",
    "            ))\n",
    "    \n",
    "    dataset.insert(dataset_records)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the moderation metric\n",
    "\n",
    "In order to evaluate the performance of the Opik moderation metric, we will define:\n",
    "\n",
    "- Evaluation task: Our evaluation task will use the data in the Dataset to return a moderation score computed using the Opik moderation metric.\n",
    "- Scoring metric: We will use the `Equals` metric to check if the moderation score computed matches the expected output.\n",
    "\n",
    "By defining the evaluation task in this way, we will be able to understand how well Opik's moderation metric is able to detect moderation violations in the dataset.\n",
    "\n",
    "We can use the Opik SDK to compute a moderation score for each item in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 50/50 [00:06<00:00,  8.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ OpenAIModerationDataset (50 samples) ─╮\n",
       "│                                        │\n",
       "│ <span style=\"font-weight: bold\">Total time:       </span> 00:00:06            │\n",
       "│ <span style=\"font-weight: bold\">Number of samples:</span> 50                  │\n",
       "│                                        │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Correct moderation score: 0.8400 (avg)</span> │\n",
       "│                                        │\n",
       "╰────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ OpenAIModerationDataset (50 samples) ─╮\n",
       "│                                        │\n",
       "│ \u001b[1mTotal time:       \u001b[0m 00:00:06            │\n",
       "│ \u001b[1mNumber of samples:\u001b[0m 50                  │\n",
       "│                                        │\n",
       "│ \u001b[1;32mCorrect moderation score: 0.8400 (avg)\u001b[0m │\n",
       "│                                        │\n",
       "╰────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Uploading results to Opik \u001b[33m...\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from opik.evaluation.metrics import Moderation, Equals\n",
    "from opik.evaluation import evaluate\n",
    "from opik import Opik, DatasetItem\n",
    "\n",
    "# Define the evaluation task\n",
    "def evaluation_task(x: DatasetItem):\n",
    "    metric = Moderation()\n",
    "    try:\n",
    "        metric_score = metric.score(\n",
    "            input= x.input[\"input\"]\n",
    "        )\n",
    "        moderation_score = metric_score.value\n",
    "        moderation_reason = metric_score.reason\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        moderation_score = None\n",
    "        moderation_reason = str(e)\n",
    "    \n",
    "    moderation_score = \"moderated\" if metric_score.value > 0.5 else \"not_moderated\"\n",
    "\n",
    "    return {\n",
    "        \"output\": moderation_score,\n",
    "        \"moderation_score\": metric_score.value,\n",
    "        \"moderation_reason\": metric_score.reason,\n",
    "        \"reference\": x.expected_output[\"expected_output\"]\n",
    "    }\n",
    "\n",
    "# Get the dataset\n",
    "client = Opik()\n",
    "dataset = client.get_dataset(name=\"OpenAIModerationDataset\")\n",
    "\n",
    "# Define the scoring metric\n",
    "moderation_metric = Equals(name=\"Correct moderation score\")\n",
    "\n",
    "res = evaluate(\n",
    "    experiment_name=\"Evaluate Opik moderation metric\",\n",
    "    dataset=dataset,\n",
    "    task=evaluation_task,\n",
    "    scoring_metrics=[moderation_metric]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to detect ~85% of moderation violations, this can be improved further by providing some additional examples to the model. We can view a breakdown of the results in the Opik UI:\n",
    "\n",
    "![Moderation Evaluation](https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/static/img/cookbook/moderation_metric_cookbook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
